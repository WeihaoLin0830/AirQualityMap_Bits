{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from pykrige.uk import UniversalKriging\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "import pyproj\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"../Dades/AMS_Observacions/\"\n",
    "crs_latlon = 'EPSG:4326'  # WGS84\n",
    "crs_utm = \"EPSG:32631\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code', 'lat', 'lon', 'type'], dtype='object')\n",
      "Index(['Date', 'ZAL', 'ES1438A', 'ES1480A', 'ES1396A', 'ES1992A', 'ES0691A',\n",
      "       'ES0692A', 'ES1148A', 'ES1679A', 'ES1856A', 'ES2090A', 'ES1453A',\n",
      "       'ES1929A', 'ES1903A', 'ES1910A', 'ES2011A', 'ES2012A', 'ES1684A',\n",
      "       'ES1231A', 'ES1891A', 'ES0584A', 'ES1262A', 'ES1018A', 'ES1551A',\n",
      "       'ES1814A', 'ES1817A', 'ES1126A', 'ES0971A', 'ES1339A', 'ES1815A',\n",
      "       'ES1773A', 'ES1899A', 'ES1122A', 'ES1123A', 'ES2109A', 'ES1124A',\n",
      "       'ES1208A', 'ES1666A', 'ES1312A', 'ES1812A', 'ES1125A', 'ES1135A',\n",
      "       'ES1642A', 'ES1397A', 'ES1923A', 'ES1816A', 'ES1999A', 'ES1347A',\n",
      "       'ES1778A', 'ES1275A', 'ES1311A', 'ES1851A', 'ES1348A', 'ES1982A',\n",
      "       'ES1225A', 'ES2034A', 'ES1215A', 'ES2017A', 'ES1930A', 'ES1948A',\n",
      "       'ES1855A', 'ES1854A', 'ES0010R', 'ES0014R'],\n",
      "      dtype='object')\n",
      "        Station  Value                Date  ZAL\n",
      "0       ES1438A    NaN 2022-12-31 23:00:00  NaN\n",
      "1       ES1438A    NaN 2022-12-31 23:00:00  NaN\n",
      "2       ES1438A    NaN 2022-12-31 23:00:00  NaN\n",
      "3       ES1438A    NaN 2022-12-31 23:00:00  NaN\n",
      "4       ES1438A    NaN 2022-12-31 23:00:00  NaN\n",
      "...         ...    ...                 ...  ...\n",
      "551875  ES0014R    NaN 2022-12-31 23:00:00  NaN\n",
      "551876  ES0014R    NaN 2022-12-31 23:00:00  NaN\n",
      "551877  ES0014R    NaN 2022-12-31 23:00:00  NaN\n",
      "551878  ES0014R    NaN 2022-12-31 23:00:00  NaN\n",
      "551879  ES0014R    NaN 2022-12-31 23:00:00  NaN\n",
      "\n",
      "[551880 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset for contaminants\n",
    "\n",
    "df_no2 = pd.read_csv('../Dades/AMS_Observacions/gene_sconcno2_2023_xvpca_emep_port.csv') \n",
    "df_estacions = pd.read_csv('../Dades/AMS_Observacions/XVPCA_info_sconcno2_2023.csv')\n",
    "\n",
    "print(df_estacions.columns)\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df_no2['Date'] = pd.to_datetime(df_no2['Date'])\n",
    "print(df_no2.columns)\n",
    "# Pivot the dataframe to have each station ID as a column and the Date as the index\n",
    "# columnas_es = [col for col in df_no2.columns if col.startswith('ES')]\n",
    "columnas_es =df_no2.drop(['Date'], axis=1)\n",
    "\n",
    "\n",
    "# Usar melt para pivotar el DataFrame\n",
    "df_melted = pd.melt(\n",
    "    df_no2, \n",
    "    value_vars=columnas_es,      # Columnas de estaciones a convertir\n",
    "    var_name='Station',          # Columna con los nombres de estaciones\n",
    "    value_name='Value'           # Columna con los valores de cada estación\n",
    ")\n",
    "\n",
    "# Agregar las demás columnas como atributos (sin repetirlas)\n",
    "df_melted['Date'] = df_no2['Date'].iloc[0]\n",
    "# df_melted['ZAL'] = df_no2['ZAL'].iloc[0]\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(df_melted)\n",
    "\n",
    "nom_estacions = ['ZAL',\n",
    "# Combina les taules utilitzant 'UniqueID' com a clau de combinació\n",
    "# combined_data = pd.merge(categorical_data, acoustic_features, on='UniqueID', how='inner'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.merged(df_melted, df_estacions, on= 'code', how = 'inner')\n",
    "\n",
    "df_comb.head()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Station                 Date   no2\n",
      "2       ES1480A  2022-12-31 23:00:00  62.0\n",
      "3       ES1396A  2022-12-31 23:00:00  60.0\n",
      "4       ES1992A  2022-12-31 23:00:00  45.0\n",
      "5       ES0691A  2022-12-31 23:00:00  43.0\n",
      "6       ES0692A  2022-12-31 23:00:00  45.0\n",
      "...         ...                  ...   ...\n",
      "560633  ES2017A  2023-12-31 22:00:00   7.0\n",
      "560634  ES1930A  2023-12-31 22:00:00   1.0\n",
      "560635  ES1948A  2023-12-31 22:00:00   3.0\n",
      "560636  ES1855A  2023-12-31 22:00:00   2.0\n",
      "560637  ES1854A  2023-12-31 22:00:00   1.0\n",
      "\n",
      "[529219 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lon'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lon'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Drop the 'code' column as it is redundant\u001b[39;00m\n\u001b[0;32m     31\u001b[0m df_no2_combined\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 32\u001b[0m df_no2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mpoints_from_xy(\u001b[43mdf_no2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, df_no2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m], crs\u001b[38;5;241m=\u001b[39mcrs_utm)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_no2_combined)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lon'"
     ]
    }
   ],
   "source": [
    "df_no2 = pd.read_csv('../Dades/AMS_Observacions/gene_sconcno2_2023_xvpca_emep_port.csv') \n",
    "df_estacions = pd.read_csv('../Dades/AMS_Observacions/XVPCA_info_sconcno2_2023.csv')\n",
    "\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df_no2['Date'] = pd.to_datetime(df_no2['Date'])\n",
    "\n",
    "# Pivot the dataframe to have each station ID as a column and the Date as the index\n",
    "# columnas_es = [col for col in df_no2.columns if col.startswith('ES')]\n",
    "columnas_es =df_no2.drop(['Date'], axis=1)\n",
    "\n",
    "\n",
    "# Transpose the dataframe to have each station ID as a row and the Date as the column\n",
    "df_no2_transposed = df_no2.set_index('Date').transpose().reset_index()\n",
    "\n",
    "# Melt the dataframe to have a 'Value' column for each hour\n",
    "df_no2_melted = pd.melt(df_no2_transposed, id_vars=['index'], var_name='Date', value_name='no2')\n",
    "\n",
    "# Rename the 'index' column to 'Station'\n",
    "df_no2_melted.rename(columns={'index': 'Station'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_no2_melted.dropna(subset=['no2'], inplace=True)\n",
    "\n",
    "print(df_no2_melted)\n",
    "\n",
    "# Join the melted dataframe with the station information dataframe\n",
    "df_obs = pd.merge(df_no2_melted, df_estacions, left_on='Station', right_on='code', how='inner')\n",
    "\n",
    "# Drop the 'code' column as it is redundant\n",
    "df_obs.drop(columns=['code','type'], inplace=True)\n",
    "df_obs['geometry'] = gpd.points_from_xy(df_obs['lon'], df_obs['lat'], crs=crs_utm)\n",
    "print(df_obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
