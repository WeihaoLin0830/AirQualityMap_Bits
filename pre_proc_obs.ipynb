{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from pykrige.uk import UniversalKriging\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "import pyproj\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"../Dades/AMS_Observacions/\"\n",
    "crs_latlon = 'EPSG:4326'  # WGS84\n",
    "crs_utm = \"EPSG:32631\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Station                 Date   no2\n",
      "2       ES1480A  2022-12-31 23:00:00  62.0\n",
      "3       ES1396A  2022-12-31 23:00:00  60.0\n",
      "4       ES1992A  2022-12-31 23:00:00  45.0\n",
      "5       ES0691A  2022-12-31 23:00:00  43.0\n",
      "6       ES0692A  2022-12-31 23:00:00  45.0\n",
      "...         ...                  ...   ...\n",
      "560633  ES2017A  2023-12-31 22:00:00   7.0\n",
      "560634  ES1930A  2023-12-31 22:00:00   1.0\n",
      "560635  ES1948A  2023-12-31 22:00:00   3.0\n",
      "560636  ES1855A  2023-12-31 22:00:00   2.0\n",
      "560637  ES1854A  2023-12-31 22:00:00   1.0\n",
      "\n",
      "[529219 rows x 3 columns]\n",
      "        Station                 Date   no2        lat       lon  \\\n",
      "0       ES1480A  2022-12-31 23:00:00  62.0  41.398762  2.153472   \n",
      "1       ES1396A  2022-12-31 23:00:00  60.0  41.378803  2.133098   \n",
      "2       ES1992A  2022-12-31 23:00:00  45.0  41.387273  2.115661   \n",
      "3       ES0691A  2022-12-31 23:00:00  43.0  41.403716  2.204736   \n",
      "4       ES0692A  2022-12-31 23:00:00  45.0  41.370760  2.114771   \n",
      "...         ...                  ...   ...        ...       ...   \n",
      "529214  ES2017A  2023-12-31 22:00:00   7.0  40.552819  0.529983   \n",
      "529215  ES1930A  2023-12-31 22:00:00   1.0  40.902693  0.809795   \n",
      "529216  ES1948A  2023-12-31 22:00:00   3.0  40.939553  0.831337   \n",
      "529217  ES1855A  2023-12-31 22:00:00   2.0  41.009506  0.912876   \n",
      "529218  ES1854A  2023-12-31 22:00:00   1.0  41.008212  0.831085   \n",
      "\n",
      "                    geometry  \n",
      "0       POINT (2.153 41.399)  \n",
      "1       POINT (2.133 41.379)  \n",
      "2       POINT (2.116 41.387)  \n",
      "3       POINT (2.205 41.404)  \n",
      "4       POINT (2.115 41.371)  \n",
      "...                      ...  \n",
      "529214   POINT (0.53 40.553)  \n",
      "529215   POINT (0.81 40.903)  \n",
      "529216   POINT (0.831 40.94)  \n",
      "529217   POINT (0.913 41.01)  \n",
      "529218  POINT (0.831 41.008)  \n",
      "\n",
      "[529219 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df_no2 = pd.read_csv('../Dades/AMS_Observacions/gene_sconcno2_2023_xvpca_emep_port.csv') \n",
    "df_estacions = pd.read_csv('../Dades/AMS_Observacions/XVPCA_info_sconcno2_2023.csv')\n",
    "\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df_no2['Date'] = pd.to_datetime(df_no2['Date'])\n",
    "\n",
    "# Pivot the dataframe to have each station ID as a column and the Date as the index\n",
    "# columnas_es = [col for col in df_no2.columns if col.startswith('ES')]\n",
    "columnas_es =df_no2.drop(['Date'], axis=1)\n",
    "\n",
    "\n",
    "# Transpose the dataframe to have each station ID as a row and the Date as the column\n",
    "df_no2_transposed = df_no2.set_index('Date').transpose().reset_index()\n",
    "\n",
    "# Melt the dataframe to have a 'Value' column for each hour\n",
    "df_no2_melted = pd.melt(df_no2_transposed, id_vars=['index'], var_name='Date', value_name='no2')\n",
    "\n",
    "# Rename the 'index' column to 'Station'\n",
    "df_no2_melted.rename(columns={'index': 'Station'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_no2_melted.dropna(subset=['no2'], inplace=True)\n",
    "\n",
    "print(df_no2_melted)\n",
    "\n",
    "# Join the melted dataframe with the station information dataframe\n",
    "df_obs = pd.merge(df_no2_melted, df_estacions, left_on='Station', right_on='code', how='inner')\n",
    "\n",
    "# Drop the 'code' column as it is redundant\n",
    "df_obs.drop(columns=['code','type'], inplace=True)\n",
    "df_obs['geometry'] = gpd.points_from_xy(df_obs['lon'], df_obs['lat'], crs=crs_utm)\n",
    "print(df_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sourceid  prioridad                                         geometry\n",
      "0  198794320   0.166667  LINESTRING (1.99488 41.66973, 1.99502 41.66971)\n",
      "1  198935562   0.333333  LINESTRING (2.23387 41.94304, 2.23386 41.94206)\n",
      "2  198944072   0.333333  LINESTRING (1.68342 41.33341, 1.68353 41.33318)\n",
      "3  198965364   0.166667  LINESTRING (2.01923 41.57089, 2.01926 41.57088)\n",
      "4  198965365   0.166667  LINESTRING (2.01857 41.57034, 2.01852 41.57033)\n"
     ]
    }
   ],
   "source": [
    "# Cargar el shapefile\n",
    "archivo_shapefile = \"../Dades/OpenTransportMap/Barcelona/roadlinks_ES511.shp\"\n",
    "gdf_barcelona = gpd.read_file(archivo_shapefile)\n",
    "gdf_girona = gpd.read_file(\"../Dades/OpenTransportMap/Girona/roadlinks_ES512.shp\")\n",
    "gdf_lleida = gpd.read_file(\"../Dades/OpenTransportMap/Lleida/roadlinks_ES513.shp\")\n",
    "gdf_tarragona = gpd.read_file(\"../Dades/OpenTransportMap/Tarragona/roadlinks_ES514.shp\")\n",
    "\n",
    "# Concatenate the GeoDataFrames\n",
    "#gdf = pd.concat([gdf_barcelona, gdf_girona, gdf_lleida, gdf_tarragona], ignore_index=True)\n",
    "\n",
    "gdf = gdf_barcelona\n",
    "\n",
    "\n",
    "# Mostrar las primeras filas de la tabla de atributos\n",
    "#print(gdf['functional'].head())\n",
    "\n",
    "# Obtener los tipos únicos de la columna 'functional'\n",
    "tipos_functional = gdf['functional'].unique()\n",
    "\n",
    "# Asignar valores de prioridad a las categorías\n",
    "valores_prioridad = {\n",
    "    'mainRoad': 6 / 6,\n",
    "    'firstClass': 5 / 6,\n",
    "    'secondClass': 4 / 6,\n",
    "    'thirdClass': 3 / 6,\n",
    "    'fourthClass': 2 / 6,\n",
    "    'fifthClass': 1 / 6,    \n",
    "}\n",
    "\n",
    "# Crear una nueva columna en el GeoDataFrame con los valores de prioridad\n",
    "gdf['prioridad'] = gdf['functional'].map(valores_prioridad)\n",
    "\n",
    "# Crear un nuevo DataFrame con las columnas 'sourceid', 'functional' y 'prioridad'\n",
    "df_transport = gdf[['sourceid', 'prioridad','geometry']].drop_duplicates().reset_index(drop=True)\n",
    "# Simplificar la geometría\n",
    "df_transport['geometry'] = df_transport['geometry'].simplify(tolerance=0.01, preserve_topology=True)\n",
    "\n",
    "print(df_transport.head())\n",
    "\n",
    "# Guardar el GeoDataFrame en un archivo CSV\n",
    "#gdf.to_csv(\"./carreteres.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains when parsing with format \"%Y-%m-%d\": \" 00:00:00\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m df_caliope \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Dades/NO2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      3\u001b[0m df_caliope\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msconcno2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m df_caliope[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_caliope\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m df_caliope[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mpoints_from_xy(df_caliope[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m], df_caliope[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m], crs\u001b[38;5;241m=\u001b[39mcrs_utm)\n\u001b[0;32m      9\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_obs, df_caliope], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    457\u001b[0m     arg,\n\u001b[0;32m    458\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:587\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unconverted data remains when parsing with format \"%Y-%m-%d\": \" 00:00:00\", at position 1. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "df_caliope = pd.read_csv('../Dades/NO2.csv') \n",
    "\n",
    "df_caliope.rename(columns={'sconcno2': 'no2', 'time': 'Date'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_caliope['geometry'] = gpd.points_from_xy(df_caliope['lon'], df_caliope['lat'], crs=crs_utm)\n",
    "\n",
    "df_combined = pd.concat([df_obs, df_caliope], ignore_index=True)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from multiprocessing import Pool\\n\\nstations = gpd.GeoDataFrame(df_obs[['Station', 'geometry']].drop_duplicates().reset_index(drop=True), geometry='geometry')\\nstations['buffer'] = stations.geometry.buffer(100)\\n\\n# Leer las carreteras desde el DataFrame\\ncarreteras = df_transport\\n\\n# Crear un índice espacial para las carreteras\\ncarreteras_sindex = carreteras.sindex\\n\\ndef encontrar_tipo_carretera(buffer):\\n    # Encontrar las carreteras que intersectan con el buffer\\n    posibles_intersecciones = list(carreteras_sindex.intersection(buffer.bounds))\\n    carreteras_intersectadas = carreteras.iloc[posibles_intersecciones]\\n    \\n    # Filtrar las carreteras que realmente intersectan con el buffer\\n    intersecciones = carreteras_intersectadas[carreteras_intersectadas.intersects(buffer)]\\n    \\n    if not intersecciones.empty:\\n        intersecciones['buffer_id'] = buffer.name\\n        return intersecciones[['buffer_id', 'sourceid', 'prioridad']]\\n    else:\\n        return pd.DataFrame(columns=['buffer_id', 'sourceid', 'prioridad'])\\n\\n# Procesar los buffers en paralelo\\nwith Pool() as pool:\\n    resultados = pool.map(encontrar_tipo_carretera, stations['buffer'])\\n\\n# Combinar los resultados\\nresultados_df = pd.concat(resultados, ignore_index=True)\\n\\n# Mostrar resultados\\nprint(resultados_df)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from multiprocessing import Pool\n",
    "\n",
    "stations = gpd.GeoDataFrame(df_obs[['Station', 'geometry']].drop_duplicates().reset_index(drop=True), geometry='geometry')\n",
    "stations['buffer'] = stations.geometry.buffer(100)\n",
    "\n",
    "# Leer las carreteras desde el DataFrame\n",
    "carreteras = df_transport\n",
    "\n",
    "# Crear un índice espacial para las carreteras\n",
    "carreteras_sindex = carreteras.sindex\n",
    "\n",
    "def encontrar_tipo_carretera(buffer):\n",
    "    # Encontrar las carreteras que intersectan con el buffer\n",
    "    posibles_intersecciones = list(carreteras_sindex.intersection(buffer.bounds))\n",
    "    carreteras_intersectadas = carreteras.iloc[posibles_intersecciones]\n",
    "    \n",
    "    # Filtrar las carreteras que realmente intersectan con el buffer\n",
    "    intersecciones = carreteras_intersectadas[carreteras_intersectadas.intersects(buffer)]\n",
    "    \n",
    "    if not intersecciones.empty:\n",
    "        intersecciones['buffer_id'] = buffer.name\n",
    "        return intersecciones[['buffer_id', 'sourceid', 'prioridad']]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['buffer_id', 'sourceid', 'prioridad'])\n",
    "\n",
    "# Procesar los buffers en paralelo\n",
    "with Pool() as pool:\n",
    "    resultados = pool.map(encontrar_tipo_carretera, stations['buffer'])\n",
    "\n",
    "# Combinar los resultados\n",
    "resultados_df = pd.concat(resultados, ignore_index=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(resultados_df)\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
