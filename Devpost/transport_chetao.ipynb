{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from pykrige.uk import UniversalKriging\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "import pyproj\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"../Dades/AMS_Observacions/\"\n",
    "crs_latlon = 'EPSG:4326'  # WGS84\n",
    "crs_utm = \"EPSG:32631\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Station                 Date   no2\n",
      "2       ES1480A  2022-12-31 23:00:00  62.0\n",
      "3       ES1396A  2022-12-31 23:00:00  60.0\n",
      "4       ES1992A  2022-12-31 23:00:00  45.0\n",
      "5       ES0691A  2022-12-31 23:00:00  43.0\n",
      "6       ES0692A  2022-12-31 23:00:00  45.0\n",
      "...         ...                  ...   ...\n",
      "560633  ES2017A  2023-12-31 22:00:00   7.0\n",
      "560634  ES1930A  2023-12-31 22:00:00   1.0\n",
      "560635  ES1948A  2023-12-31 22:00:00   3.0\n",
      "560636  ES1855A  2023-12-31 22:00:00   2.0\n",
      "560637  ES1854A  2023-12-31 22:00:00   1.0\n",
      "\n",
      "[529219 rows x 3 columns]\n",
      "        Station                 Date   no2        lat       lon  \\\n",
      "0       ES1480A  2022-12-31 23:00:00  62.0  41.398762  2.153472   \n",
      "1       ES1396A  2022-12-31 23:00:00  60.0  41.378803  2.133098   \n",
      "2       ES1992A  2022-12-31 23:00:00  45.0  41.387273  2.115661   \n",
      "3       ES0691A  2022-12-31 23:00:00  43.0  41.403716  2.204736   \n",
      "4       ES0692A  2022-12-31 23:00:00  45.0  41.370760  2.114771   \n",
      "...         ...                  ...   ...        ...       ...   \n",
      "529214  ES2017A  2023-12-31 22:00:00   7.0  40.552819  0.529983   \n",
      "529215  ES1930A  2023-12-31 22:00:00   1.0  40.902693  0.809795   \n",
      "529216  ES1948A  2023-12-31 22:00:00   3.0  40.939553  0.831337   \n",
      "529217  ES1855A  2023-12-31 22:00:00   2.0  41.009506  0.912876   \n",
      "529218  ES1854A  2023-12-31 22:00:00   1.0  41.008212  0.831085   \n",
      "\n",
      "                    geometry  \n",
      "0       POINT (2.153 41.399)  \n",
      "1       POINT (2.133 41.379)  \n",
      "2       POINT (2.116 41.387)  \n",
      "3       POINT (2.205 41.404)  \n",
      "4       POINT (2.115 41.371)  \n",
      "...                      ...  \n",
      "529214   POINT (0.53 40.553)  \n",
      "529215   POINT (0.81 40.903)  \n",
      "529216   POINT (0.831 40.94)  \n",
      "529217   POINT (0.913 41.01)  \n",
      "529218  POINT (0.831 41.008)  \n",
      "\n",
      "[529219 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df_no2 = pd.read_csv('../Dades/AMS_Observacions/gene_sconcno2_2023_xvpca_emep_port.csv') \n",
    "df_estacions = pd.read_csv('../Dades/AMS_Observacions/XVPCA_info_sconcno2_2023.csv')\n",
    "\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df_no2['Date'] = pd.to_datetime(df_no2['Date'])\n",
    "\n",
    "# Pivot the dataframe to have each station ID as a column and the Date as the index\n",
    "# columnas_es = [col for col in df_no2.columns if col.startswith('ES')]\n",
    "columnas_es =df_no2.drop(['Date'], axis=1)\n",
    "\n",
    "\n",
    "# Transpose the dataframe to have each station ID as a row and the Date as the column\n",
    "df_no2_transposed = df_no2.set_index('Date').transpose().reset_index()\n",
    "\n",
    "# Melt the dataframe to have a 'Value' column for each hour\n",
    "df_no2_melted = pd.melt(df_no2_transposed, id_vars=['index'], var_name='Date', value_name='no2')\n",
    "\n",
    "# Rename the 'index' column to 'Station'\n",
    "df_no2_melted.rename(columns={'index': 'Station'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_no2_melted.dropna(subset=['no2'], inplace=True)\n",
    "\n",
    "print(df_no2_melted)\n",
    "\n",
    "# Join the melted dataframe with the station information dataframe\n",
    "df_obs = pd.merge(df_no2_melted, df_estacions, left_on='Station', right_on='code', how='inner')\n",
    "\n",
    "# Drop the 'code' column as it is redundant\n",
    "df_obs.drop(columns=['code','type'], inplace=True)\n",
    "df_obs['geometry'] = gpd.points_from_xy(df_obs['lon'], df_obs['lat'], crs=crs_utm)\n",
    "print(df_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Date</th>\n",
       "      <th>no2</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES1480A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>41.398762</td>\n",
       "      <td>2.153472</td>\n",
       "      <td>POINT (2.153 41.399)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES1396A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>41.378803</td>\n",
       "      <td>2.133098</td>\n",
       "      <td>POINT (2.133 41.379)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES1992A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>41.387273</td>\n",
       "      <td>2.115661</td>\n",
       "      <td>POINT (2.116 41.387)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES0691A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>41.403716</td>\n",
       "      <td>2.204736</td>\n",
       "      <td>POINT (2.205 41.404)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ES0692A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>41.370760</td>\n",
       "      <td>2.114771</td>\n",
       "      <td>POINT (2.115 41.371)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Station                 Date   no2        lat       lon  \\\n",
       "0  ES1480A  2022-12-31 23:00:00  62.0  41.398762  2.153472   \n",
       "1  ES1396A  2022-12-31 23:00:00  60.0  41.378803  2.133098   \n",
       "2  ES1992A  2022-12-31 23:00:00  45.0  41.387273  2.115661   \n",
       "3  ES0691A  2022-12-31 23:00:00  43.0  41.403716  2.204736   \n",
       "4  ES0692A  2022-12-31 23:00:00  45.0  41.370760  2.114771   \n",
       "\n",
       "               geometry  \n",
       "0  POINT (2.153 41.399)  \n",
       "1  POINT (2.133 41.379)  \n",
       "2  POINT (2.116 41.387)  \n",
       "3  POINT (2.205 41.404)  \n",
       "4  POINT (2.115 41.371)  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caliope = pd.read_csv('./NO2_bo_ppm.csv') \n",
    "\n",
    "df_caliope.rename(columns={'sconcno2': 'no2', 'time': 'Date'}, inplace=True)\n",
    "\n",
    "df_caliope['geometry'] = gpd.points_from_xy(df_caliope['lon'], df_caliope['lat'], crs=crs_utm)\n",
    "\n",
    "df_combined = pd.concat([df_obs, df_caliope], ignore_index=True)\n",
    "\n",
    "# Convert df_combined to a GeoDataFrame\n",
    "df_combined = gpd.GeoDataFrame(df_combined, geometry='geometry', crs=crs_utm)\n",
    "\n",
    "# Display the first few rows to verify the conversion\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max NO2: 61.54424753333333\n",
      "Min NO2: 0.14694943113333334\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por 'geometry' y calcular la media de 'no2'\n",
    "df_combined_mean = df_caliope.groupby('geometry')['no2'].mean().reset_index()\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(\"Max NO2:\", df_combined_mean['no2'].max())\n",
    "print(\"Min NO2:\", df_combined_mean['no2'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               geometry  predicted_value\n",
      "0   POINT (2.01 41.392)        11.074402\n",
      "1  POINT (1.192 41.116)        18.137966\n",
      "2  POINT (2.238 41.444)        18.646513\n",
      "3  POINT (2.082 41.322)        16.326666\n",
      "       Station                 Date   no2        lat       lon  \\\n",
      "0      ES1480A  2022-12-31 23:00:00  62.0  41.398762  2.153472   \n",
      "1      ES1396A  2022-12-31 23:00:00  60.0  41.378803  2.133098   \n",
      "2      ES1992A  2022-12-31 23:00:00  45.0  41.387273  2.115661   \n",
      "3      ES0691A  2022-12-31 23:00:00  43.0  41.403716  2.204736   \n",
      "4      ES0692A  2022-12-31 23:00:00  45.0  41.370760  2.114771   \n",
      "...        ...                  ...   ...        ...       ...   \n",
      "2203   ES1348A  2023-01-02 12:00:00  16.0  42.368390  1.776814   \n",
      "3579   ES1438A  2023-01-03 12:00:00  35.0  41.385366  2.154030   \n",
      "12183  ES1815A  2023-01-09 10:00:00   3.0  41.346823  1.686575   \n",
      "13418  ES1854A  2023-01-10 06:00:00   1.0  41.008212  0.831085   \n",
      "86441  ES1642A  2023-02-28 23:00:00   6.0  41.935010  2.239901   \n",
      "\n",
      "                   geometry  predicted_value  \n",
      "0      POINT (2.153 41.399)        15.889915  \n",
      "1      POINT (2.133 41.379)        22.328926  \n",
      "2      POINT (2.116 41.387)        14.030311  \n",
      "3      POINT (2.205 41.404)        17.700815  \n",
      "4      POINT (2.115 41.371)        17.791494  \n",
      "...                     ...              ...  \n",
      "2203   POINT (1.777 42.368)         2.289370  \n",
      "3579   POINT (2.154 41.385)        17.897125  \n",
      "12183  POINT (1.687 41.347)         7.751471  \n",
      "13418  POINT (0.831 41.008)         3.246053  \n",
      "86441   POINT (2.24 41.935)         8.186480  \n",
      "\n",
      "[64 rows x 7 columns]\n",
      "   Station                 Date   no2        lat       lon  \\\n",
      "0  ES1480A  2022-12-31 23:00:00  62.0  41.398762  2.153472   \n",
      "1  ES1396A  2022-12-31 23:00:00  60.0  41.378803  2.133098   \n",
      "2  ES1992A  2022-12-31 23:00:00  45.0  41.387273  2.115661   \n",
      "3  ES0691A  2022-12-31 23:00:00  43.0  41.403716  2.204736   \n",
      "4  ES0692A  2022-12-31 23:00:00  45.0  41.370760  2.114771   \n",
      "\n",
      "               geometry  predicted_value  \n",
      "0  POINT (2.153 41.399)        15.889915  \n",
      "1  POINT (2.133 41.379)        22.328926  \n",
      "2  POINT (2.116 41.387)        14.030311  \n",
      "3  POINT (2.205 41.404)        17.700815  \n",
      "4  POINT (2.115 41.371)        17.791494  \n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "stations_gdf = gpd.GeoDataFrame(df_obs.drop_duplicates(subset=['geometry']).copy(), geometry='geometry')\n",
    "# Crear GeoDataFrame con puntos de predicción\n",
    "obs_locations = gpd.GeoDataFrame(\n",
    "    {'geometry': [Point(2.009802, 41.39216), Point(1.191975, 41.11588), \n",
    "                  Point(2.237875, 41.44398), Point(2.082141, 41.32177)]},\n",
    "    crs=crs_utm\n",
    ")\n",
    "\n",
    "# Extraer coordenadas y valores del dataset conocido\n",
    "coords = np.array([(geom.x, geom.y) for geom in df_combined_mean.geometry])\n",
    "values = df_combined_mean['no2'].values\n",
    "\n",
    "# Crear el árbol KD\n",
    "tree = cKDTree(coords)\n",
    "\n",
    "# Función IDW\n",
    "def idw(x, y, tree, coords, values, power=2):\n",
    "    \"\"\"\n",
    "    Interpolación por inverso de la distancia (IDW).\n",
    "    - x, y: Coordenadas de predicción\n",
    "    - tree: KDTree construido con los datos conocidos\n",
    "    - coords: Coordenadas conocidas\n",
    "    - values: Valores conocidos\n",
    "    - power: Peso de la distancia\n",
    "    \"\"\"\n",
    "    # Encontrar distancias y vecinos\n",
    "    distances, idx = tree.query(np.c_[x, y], k=len(coords))\n",
    "    \n",
    "    # Manejo de distancias 0 (puntos coincidentes)\n",
    "    distances[distances == 0] = 1e-10  # Evitar división por cero\n",
    "    \n",
    "    # Calcular pesos inversos a las distancias\n",
    "    weights = 1 / distances**power\n",
    "    \n",
    "    # Normalizar los pesos\n",
    "    weights /= np.sum(weights, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calcular valores interpolados\n",
    "    interpolated_values = np.sum(weights * values[idx], axis=1)\n",
    "    return interpolated_values\n",
    "\n",
    "# Coordenadas de los puntos a predecir\n",
    "prediction_coords = np.array([(geom.x, geom.y) for geom in obs_locations.geometry])\n",
    "x_pred, y_pred = prediction_coords[:, 0], prediction_coords[:, 1]\n",
    "\n",
    "# Aplicar IDW a los puntos\n",
    "obs_locations['predicted_value'] = idw(x_pred, y_pred, tree, coords, values)\n",
    "\n",
    "print(obs_locations)\n",
    "\n",
    "\n",
    "grid_gdf = pd.read_csv('grid_gdf.csv')\n",
    "\n",
    "# Ensure the geometry column is properly set as a GeoSeries\n",
    "grid_gdf['geometry'] = gpd.GeoSeries.from_wkt(grid_gdf['geometry'])\n",
    "grid_gdf = gpd.GeoDataFrame(grid_gdf, geometry='geometry', crs=crs_utm)\n",
    "\n",
    "grid_gdf.drop(columns=['kriging_variance'], inplace=True)\n",
    "grid_gdf.rename(columns={'kriging_prediction': 'predicted_value'}, inplace=True)\n",
    "\n",
    "resultats_station = grid_gdf.copy()\n",
    "\n",
    "# Coordenadas de los puntos de las estaciones\n",
    "station_coords = np.array([(geom.x, geom.y) for geom in stations_gdf.geometry])\n",
    "x_station, y_station = station_coords[:, 0], station_coords[:, 1]\n",
    "\n",
    "# Aplicar IDW a los puntos de las estaciones\n",
    "stations_gdf['predicted_value'] = idw(x_station, y_station, tree, coords, values)\n",
    "\n",
    "print(stations_gdf)\n",
    "print(stations_gdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sourceid  prioridad                                           geometry\n",
      "8   247323996       0.25  LINESTRING (2.229 41.66569, 2.229 41.66606, 2....\n",
      "10  199276193       0.25  LINESTRING (1.91614 41.5961, 1.91615 41.59607,...\n",
      "11  199276720       1.00    LINESTRING (1.63758 41.73453, 1.63799 41.73411)\n",
      "13  199316496       1.00  LINESTRING (1.92758 41.79895, 1.92656 41.7982,...\n",
      "14  199333176       1.00  LINESTRING (2.16105 41.93252, 2.16175 41.93245...\n"
     ]
    }
   ],
   "source": [
    "# Cargar el shapefile\n",
    "archivo_shapefile = \"../Dades/OpenTransportMap/Barcelona/roadlinks_ES511.shp\"\n",
    "gdf_barcelona = gpd.read_file(archivo_shapefile)\n",
    "gdf_girona = gpd.read_file(\"../Dades/OpenTransportMap/Girona/roadlinks_ES512.shp\")\n",
    "gdf_lleida = gpd.read_file(\"../Dades/OpenTransportMap/Lleida/roadlinks_ES513.shp\")\n",
    "gdf_tarragona = gpd.read_file(\"../Dades/OpenTransportMap/Tarragona/roadlinks_ES514.shp\")\n",
    "\n",
    "# Concatenate the GeoDataFrames\n",
    "gdf = pd.concat([gdf_barcelona, gdf_girona, gdf_lleida, gdf_tarragona], ignore_index=True)\n",
    "# Mostrar las primeras filas de la tabla de atributos\n",
    "#print(gdf['functional'].head())\n",
    "\n",
    "# Obtener los tipos únicos de la columna 'functional'\n",
    "tipos_functional = gdf['functional'].unique()\n",
    "\n",
    "# Asignar valores de prioridad a las categorías\n",
    "valores_prioridad = {\n",
    "    'mainRoad': 4 / 4,\n",
    "    'firstClass': 3 / 4,\n",
    "    'secondClass': 2 / 4,\n",
    "    'thirdClass': 1 / 4,\n",
    "    'fourthClass': 0,\n",
    "    'fifthClass': 0,    \n",
    "}\n",
    "\n",
    "# Crear una nueva columna en el GeoDataFrame con los valores de prioridad\n",
    "gdf['prioridad'] = gdf['functional'].map(valores_prioridad)\n",
    "\n",
    "# Crear un nuevo DataFrame con las columnas 'sourceid', 'functional' y 'prioridad'\n",
    "df_transport = gdf[['sourceid', 'prioridad','geometry']].drop_duplicates().reset_index(drop=True)\n",
    "# Eliminar filas con valores de prioridad igual a 0\n",
    "df_transport = df_transport[df_transport['prioridad'] != 0]\n",
    "print(df_transport.head())\n",
    "\n",
    "# Guardar el GeoDataFrame en un archivo CSV\n",
    "#gdf.to_csv(\"./carreteres.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "roads_gdf = gpd.GeoDataFrame(df_transport.copy(), geometry='geometry')\n",
    "\n",
    "stations_gdf = stations_gdf[stations_gdf.geometry.is_valid]\n",
    "roads_gdf = roads_gdf[roads_gdf.geometry.is_valid]\n",
    "\n",
    "def encontrar_carretera_cercana(punto):\n",
    "    # Busca los índices de las carreteras cercanas\n",
    "    posibles_indices = list(roads_gdf.sindex.intersection(punto.buffer(0.01).bounds))\n",
    "    \n",
    "    # Si hay carreteras cercanas\n",
    "    if posibles_indices:\n",
    "        # Filtrar y encontrar la más cercana\n",
    "        carreteras_cercanas = roads_gdf.iloc[posibles_indices]\n",
    "        distancias = carreteras_cercanas.geometry.distance(punto)\n",
    "        indice_mas_cercano = distancias.idxmin()\n",
    "        road = roads_gdf.loc[indice_mas_cercano]\n",
    "        return road['prioridad']\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               geometry  carretera_cercana   no2\n",
      "0  POINT (2.153 41.399)               0.75  62.0\n",
      "1  POINT (2.133 41.379)               0.25  60.0\n",
      "2  POINT (2.116 41.387)               0.25  45.0\n",
      "3  POINT (2.205 41.404)               0.25  43.0\n",
      "4  POINT (2.115 41.371)               0.25  45.0\n"
     ]
    }
   ],
   "source": [
    "# Aplicar la función a cada punto\n",
    "import warnings\n",
    "\n",
    "# Aplicar la función a cada punto en stations_gdf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stations_gdf['carretera_cercana'] = stations_gdf['geometry'].apply(encontrar_carretera_cercana)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(stations_gdf[['geometry', 'carretera_cercana','no2']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sourceid': 22700267, 'prioridad': 1.0, 'distancia': 0.010075922733582246}, {'sourceid': 22700252, 'prioridad': 1.0, 'distancia': 0.010427126711956109}, {'sourceid': 180601447, 'prioridad': 1.0, 'distancia': 0.010465589547823388}, {'sourceid': 224996037, 'prioridad': 1.0, 'distancia': 0.010467810872170736}, {'sourceid': 22700287, 'prioridad': 1.0, 'distancia': 0.010619678859567187}, {'sourceid': 180601446, 'prioridad': 1.0, 'distancia': 0.010621203972379034}, {'sourceid': 104695559, 'prioridad': 1.0, 'distancia': 0.010809408173949658}, {'sourceid': 180601446, 'prioridad': 1.0, 'distancia': 0.01082981794235107}, {'sourceid': 22700252, 'prioridad': 1.0, 'distancia': 0.010835340013524973}, {'sourceid': 22700288, 'prioridad': 1.0, 'distancia': 0.011152859328482272}]\n",
      "   Station                 Date   no2        lat       lon  \\\n",
      "0  ES1480A  2022-12-31 23:00:00  62.0  41.398762  2.153472   \n",
      "1  ES1396A  2022-12-31 23:00:00  60.0  41.378803  2.133098   \n",
      "2  ES1992A  2022-12-31 23:00:00  45.0  41.387273  2.115661   \n",
      "3  ES0691A  2022-12-31 23:00:00  43.0  41.403716  2.204736   \n",
      "4  ES0692A  2022-12-31 23:00:00  45.0  41.370760  2.114771   \n",
      "\n",
      "               geometry  predicted_value  carretera_cercana  \\\n",
      "0  POINT (2.153 41.399)        15.889915               0.75   \n",
      "1  POINT (2.133 41.379)        22.328926               0.25   \n",
      "2  POINT (2.116 41.387)        14.030311               0.25   \n",
      "3  POINT (2.205 41.404)        17.700815               0.25   \n",
      "4  POINT (2.115 41.371)        17.791494               0.25   \n",
      "\n",
      "                                           road_dict  \n",
      "0  [{'sourceid': 237659863, 'prioridad': 0.75, 'd...  \n",
      "1  [{'sourceid': 239293914, 'prioridad': 0.25, 'd...  \n",
      "2  [{'sourceid': 34038369, 'prioridad': 0.25, 'di...  \n",
      "3  [{'sourceid': 237787025, 'prioridad': 0.25, 'd...  \n",
      "4  [{'sourceid': 80789111, 'prioridad': 0.25, 'di...  \n",
      "[{'sourceid': 237659863, 'prioridad': 0.75, 'distancia': 0.0003040021305667805}, {'sourceid': 35159839, 'prioridad': 0.75, 'distancia': 0.0003119492426666281}, {'sourceid': 165524143, 'prioridad': 0.75, 'distancia': 0.0004299788963226757}, {'sourceid': 288454797, 'prioridad': 0.75, 'distancia': 0.00043339914628254494}, {'sourceid': 251698848, 'prioridad': 0.75, 'distancia': 0.0011283693101114595}, {'sourceid': 35159839, 'prioridad': 0.75, 'distancia': 0.0012153769867775945}, {'sourceid': 288454797, 'prioridad': 0.75, 'distancia': 0.001221350617140175}, {'sourceid': 288454793, 'prioridad': 0.75, 'distancia': 0.0016528890585902846}, {'sourceid': 165524143, 'prioridad': 0.75, 'distancia': 0.0017118832319983183}, {'sourceid': 288454797, 'prioridad': 0.75, 'distancia': 0.0017470243615899164}]\n",
      "0        62.0\n",
      "1        60.0\n",
      "2        45.0\n",
      "3        43.0\n",
      "4        45.0\n",
      "         ... \n",
      "2203     16.0\n",
      "3579     35.0\n",
      "12183     3.0\n",
      "13418     1.0\n",
      "86441     6.0\n",
      "Name: no2, Length: 64, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def encontrar_road_dict(punto, carreteras_gdf, buffer_distance=0.01):\n",
    "    # Crear un buffer alrededor del punto\n",
    "    punto_buffer = punto.buffer(buffer_distance)\n",
    "    \n",
    "    # Encontrar carreteras dentro del buffer\n",
    "    road_dict = carreteras_gdf[carreteras_gdf.intersects(punto_buffer)]\n",
    "    \n",
    "    # Si no hay carreteras cercanas, devolver None\n",
    "    if road_dict.empty:\n",
    "        return None\n",
    "    \n",
    "    # Calcular distancias y otra información\n",
    "    road_dict['distancia'] = road_dict.geometry.apply(lambda geom: geom.distance(punto))\n",
    "    \n",
    "    # Ordenar por distancia\n",
    "    road_dict = road_dict.sort_values('distancia')\n",
    "    \n",
    "    # Extraer información relevante\n",
    "    # Eliminar duplicados basados en la distancia\n",
    "    road_dict = road_dict.drop_duplicates(subset=['distancia']).head(10)\n",
    "    \n",
    "    info_carreteras = road_dict.apply(lambda row: {\n",
    "        'sourceid': row['sourceid'],\n",
    "        'prioridad': row['prioridad'],\n",
    "        'distancia': row['distancia']\n",
    "    }, axis=1).tolist()\n",
    "    \n",
    "    return info_carreteras\n",
    "\n",
    "# Aplicar la función a cada punto\n",
    "def procesar_estaciones(estaciones_gdf, buffer_size=0.05):\n",
    "    # Crear una copia para no modificar el original\n",
    "    estaciones_procesadas = estaciones_gdf.copy()\n",
    "    \n",
    "    # Aplicar la función de búsqueda de carreteras cercanas\n",
    "    estaciones_procesadas['road_dict'] = estaciones_procesadas.geometry.apply(\n",
    "        lambda punto: encontrar_road_dict(punto, roads_gdf, buffer_distance=buffer_size)\n",
    "    )\n",
    "    \n",
    "    return estaciones_procesadas\n",
    "\n",
    "# Ejecutar el procesamiento\n",
    "stations_gdf = procesar_estaciones(stations_gdf)\n",
    "\n",
    "resultats_station = procesar_estaciones(resultats_station)\n",
    "\n",
    "print(resultats_station['road_dict'].iloc[0])\n",
    "print(stations_gdf.head())\n",
    "print(stations_gdf['road_dict'].iloc[0])\n",
    "\n",
    "print(stations_gdf['no2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prioridad_media  prioridad_min  prioridad_max  distancia_media  \\\n",
      "0             0.75           0.75           0.75         0.001016   \n",
      "1             0.35           0.25           0.75         0.002787   \n",
      "2             0.40           0.25           0.75         0.001980   \n",
      "3             0.25           0.25           0.25         0.001888   \n",
      "4             0.25           0.25           0.25         0.001177   \n",
      "\n",
      "   distancia_min   no2  predicted_value  \n",
      "0       0.000304  62.0        15.889915  \n",
      "1       0.002102  60.0        22.328926  \n",
      "2       0.001486  45.0        14.030311  \n",
      "3       0.001422  43.0        17.700815  \n",
      "4       0.000299  45.0        17.791494  \n",
      "   prioridad_media  prioridad_min  prioridad_max  distancia_media  \\\n",
      "0              1.0            1.0            1.0         0.010630   \n",
      "1              1.0            1.0            1.0         0.009230   \n",
      "2              1.0            1.0            1.0         0.007325   \n",
      "3              1.0            1.0            1.0         0.005438   \n",
      "4              1.0            1.0            1.0         0.003665   \n",
      "\n",
      "   distancia_min              geometry  predicted_value  \n",
      "0       0.010076  POINT (2.054 41.318)        34.289335  \n",
      "1       0.008512  POINT (2.054 41.319)        35.351719  \n",
      "2       0.006331  POINT (2.054 41.321)        36.836688  \n",
      "3       0.004224  POINT (2.054 41.324)        38.294711  \n",
      "4       0.002339  POINT (2.054 41.326)        39.726016  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_station = stations_gdf[['geometry', 'predicted_value', 'road_dict', 'no2']].copy()\n",
    "df_station.head()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Preparar datos para el modelo\n",
    "def extraer_caracteristicas(road_dict):\n",
    "    if not road_dict or len(road_dict) == 0:\n",
    "        return {\n",
    "            'num_carreteras': 0,\n",
    "            'prioridad_media': 0,\n",
    "            'prioridad_min': 0,\n",
    "            'prioridad_max': 0,\n",
    "            'distancia_media': 0,\n",
    "            'distancia_min': 0\n",
    "        }\n",
    "    \n",
    "    # Extraer información de las carreteras\n",
    "    num_carreteras = len(road_dict)\n",
    "    prioridades = [road['prioridad'] for road in road_dict]\n",
    "    distancias = [road['distancia'] for road in road_dict]\n",
    "    \n",
    "    return {\n",
    "        'num_carreteras': num_carreteras,\n",
    "        'prioridad_media': np.mean(prioridades),\n",
    "        'prioridad_min': np.min(prioridades),\n",
    "        'prioridad_max': np.max(prioridades),\n",
    "        'distancia_media': np.mean(distancias),\n",
    "        'distancia_min': np.min(distancias)\n",
    "    }\n",
    "\n",
    "# Preparar el DataFrame de características\n",
    "def preparar_datos(df_station):\n",
    "    # Extraer características de las carreteras\n",
    "    \n",
    "    caracteristicas_carreteras = df_station['road_dict'].apply(extraer_caracteristicas)\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    df_caracteristicas = pd.DataFrame(caracteristicas_carreteras.tolist())\n",
    "    \n",
    "    # Añadir la columna de NO2\n",
    "    if 'no2' in df_station.columns:\n",
    "        df_caracteristicas['no2'] = df_station['no2']\n",
    "    else:\n",
    "        df_caracteristicas['geometry'] = df_station['geometry']\n",
    "    df_caracteristicas['predicted_value'] = df_station['predicted_value']\n",
    "    \n",
    "    return df_caracteristicas\n",
    "\n",
    "dades_forest = preparar_datos(df_station).drop(columns=['num_carreteras'])\n",
    "resultat_categoriques = preparar_datos(resultats_station).drop(columns=['num_carreteras'])\n",
    "\n",
    "print(dades_forest.head())\n",
    "print(resultat_categoriques.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prioridad_media  prioridad_min  prioridad_max  distancia_media  \\\n",
      "0             0.750           0.75           0.75         0.001016   \n",
      "1             0.350           0.25           0.75         0.002787   \n",
      "2             0.400           0.25           0.75         0.001980   \n",
      "3             0.250           0.25           0.25         0.001888   \n",
      "4             0.250           0.25           0.25         0.001177   \n",
      "5             0.250           0.25           0.25         0.001952   \n",
      "6             0.475           0.25           0.50         0.001742   \n",
      "7             0.250           0.25           0.25         0.003858   \n",
      "8             0.250           0.25           0.25         0.004328   \n",
      "9             0.250           0.25           0.25         0.001499   \n",
      "10            0.250           0.25           0.25         0.000659   \n",
      "11            0.250           0.25           0.25         0.004448   \n",
      "12            0.250           0.25           0.25         0.002150   \n",
      "13            0.750           0.75           0.75         0.004643   \n",
      "14            1.000           1.00           1.00         0.006829   \n",
      "15            0.500           0.50           0.50         0.003920   \n",
      "16            0.250           0.25           0.25         0.003308   \n",
      "17            0.275           0.25           0.50         0.000969   \n",
      "18            0.850           0.25           1.00         0.003625   \n",
      "19            0.250           0.25           0.25         0.000897   \n",
      "20            0.475           0.25           0.50         0.000342   \n",
      "21            0.250           0.25           0.25         0.000882   \n",
      "22            1.000           1.00           1.00         0.002612   \n",
      "23            0.500           0.50           0.50         0.000370   \n",
      "24            0.500           0.50           0.50         0.001134   \n",
      "25            1.000           1.00           1.00         0.001629   \n",
      "26            0.250           0.25           0.25         0.000452   \n",
      "27            0.750           0.75           0.75         0.005113   \n",
      "28            0.250           0.25           0.25         0.001855   \n",
      "29            0.750           0.75           0.75         0.006267   \n",
      "30            0.250           0.25           0.25         0.004091   \n",
      "31            0.625           0.25           1.00         0.001771   \n",
      "32            1.000           1.00           1.00         0.001051   \n",
      "33            0.750           0.75           0.75         0.001702   \n",
      "34            0.750           0.75           0.75         0.002850   \n",
      "35            0.775           0.25           1.00         0.003893   \n",
      "36            0.750           0.75           0.75         0.002587   \n",
      "37            0.250           0.25           0.25         0.002861   \n",
      "38            0.250           0.25           0.25         0.002419   \n",
      "39            0.250           0.25           0.25         0.001543   \n",
      "40            0.250           0.25           0.25         0.002585   \n",
      "41            0.250           0.25           0.25         0.002429   \n",
      "42            0.500           0.50           0.50         0.000786   \n",
      "43            0.500           0.50           0.50         0.015251   \n",
      "44            0.250           0.25           0.25         0.023762   \n",
      "45            0.350           0.25           0.75         0.001923   \n",
      "46            0.250           0.25           0.25         0.001873   \n",
      "47            0.500           0.50           0.50         0.049571   \n",
      "48            0.750           0.75           0.75         0.002302   \n",
      "49            0.750           0.75           0.75         0.003657   \n",
      "50            0.250           0.25           0.25         0.002066   \n",
      "51            0.750           0.75           0.75         0.005306   \n",
      "52            0.800           0.75           1.00         0.004121   \n",
      "53            0.550           0.25           0.75         0.008739   \n",
      "54            0.750           0.75           0.75         0.002491   \n",
      "55            0.675           0.50           0.75         0.005705   \n",
      "\n",
      "    predicted_value  distancia_min  \n",
      "0         15.889915       0.000304  \n",
      "1         22.328926       0.002102  \n",
      "2         14.030311       0.001486  \n",
      "3         17.700815       0.001422  \n",
      "4         17.791494       0.000299  \n",
      "5         20.443290       0.001836  \n",
      "6         19.365926       0.001508  \n",
      "7         12.414928       0.002446  \n",
      "8         10.817179       0.003416  \n",
      "9         19.737024       0.001043  \n",
      "10        16.571924       0.000312  \n",
      "11        12.713695       0.003300  \n",
      "12        11.411841       0.001718  \n",
      "13        10.977775       0.003857  \n",
      "14         9.797177       0.003180  \n",
      "15        11.968581       0.003716  \n",
      "16        12.467926       0.003132  \n",
      "17        11.663389       0.000247  \n",
      "18        15.745879       0.002462  \n",
      "19        17.177323       0.000087  \n",
      "20        10.351223       0.000108  \n",
      "21        14.738761       0.000725  \n",
      "22        14.447171       0.000890  \n",
      "23        14.371965       0.000283  \n",
      "24         7.720767       0.000261  \n",
      "25         9.371841       0.001194  \n",
      "26         8.040577       0.000357  \n",
      "27         7.549447       0.004848  \n",
      "28         9.301209       0.000920  \n",
      "29        11.795619       0.006118  \n",
      "30        16.983297       0.003757  \n",
      "31        11.550923       0.001600  \n",
      "32        12.382085       0.000171  \n",
      "33         9.421618       0.000356  \n",
      "34        23.410597       0.001019  \n",
      "35        19.203186       0.001055  \n",
      "36         4.989758       0.000832  \n",
      "37         8.357501       0.002506  \n",
      "38         5.506160       0.000738  \n",
      "39         7.773633       0.001250  \n",
      "40         4.258036       0.001859  \n",
      "41         9.269077       0.001997  \n",
      "42         8.891228       0.000366  \n",
      "43         2.829678       0.012544  \n",
      "44         2.592467       0.015483  \n",
      "45         5.178233       0.001343  \n",
      "46         4.152536       0.001579  \n",
      "47         1.079681       0.049268  \n",
      "48         7.396743       0.002198  \n",
      "49         7.819567       0.003504  \n",
      "50         4.322641       0.001977  \n",
      "51         4.697202       0.005274  \n",
      "52         4.313366       0.003641  \n",
      "53         4.180673       0.001589  \n",
      "54         5.162458       0.002256  \n",
      "55        19.074344       0.005390  \n",
      "Random Forest Mean Squared Error: 128.20839285714285\n",
      "Random Forest R² Score: 0.4849180366721958\n",
      "Random Forest Root Mean Squared Error: 11.322914503657742\n",
      "Predicciones: [34.25 39.05 33.3  36.45 36.45 37.25 34.75 29.9  26.4  38.35 37.3  28.8\n",
      " 26.4  26.2  21.1  25.   29.9  25.   33.05 37.5  22.4  33.5  32.   38.2\n",
      " 20.5  21.1  21.   19.2  20.4  26.7  34.75 24.9  26.3  25.1  39.05 42.25\n",
      " 12.9  21.   15.1  21.    8.8  20.4  22.8   4.6   8.   11.2   8.8   4.4\n",
      " 17.5  19.2   8.8  10.5  10.5   7.5  12.9  37.45]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Asegúrate de que no hay NaN en los datos\n",
    "# Definir las características y la variable objetivo\n",
    "features = [ 'prioridad_media', 'prioridad_min', 'prioridad_max', \n",
    "            'distancia_media', 'predicted_value','distancia_min']\n",
    "dades_forest = dades_forest.dropna(subset=features + ['no2'])\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_features = dades_forest[features].copy()\n",
    "print(X_features)\n",
    "X_features[['prioridad_media', 'prioridad_min', 'prioridad_max', 'distancia_media','distancia_min']] = scaler.fit_transform(X_features[[ 'prioridad_media', 'prioridad_min', 'prioridad_max', 'distancia_media','distancia_min']])\n",
    "y = dades_forest['no2']\n",
    "\n",
    "\n",
    "resultat_categoriques[[ 'prioridad_media', 'prioridad_min', 'prioridad_max', 'distancia_media','distancia_min' ]] = scaler.transform(resultat_categoriques[[ 'prioridad_media', 'prioridad_min', 'prioridad_max', 'distancia_media','distancia_min']])\n",
    "\n",
    "\n",
    "# Entrenar el modelo con todos los datos\n",
    "#rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "#rf_model.fit(X_features, y)\n",
    "\n",
    "rf_model = KNeighborsRegressor(n_neighbors=10)  # k=5 como valor inicial\n",
    "rf_model.fit(X_features, y)\n",
    "\n",
    "# Predecir y evaluar con los mismos datos de entrenamiento\n",
    "y_pred_rf = rf_model.predict(X_features)\n",
    "\n",
    "# Métricas de evaluación\n",
    "mse_rf = mean_squared_error(y, y_pred_rf)\n",
    "r2_rf = r2_score(y, y_pred_rf)\n",
    "\n",
    "# Calcular RMSE\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Random Forest Mean Squared Error:\", mse_rf)\n",
    "print(\"Random Forest R² Score:\", r2_rf)\n",
    "print(\"Random Forest Root Mean Squared Error:\", rmse_rf)\n",
    "\n",
    "# Imprimir las predicciones\n",
    "print(\"Predicciones:\", y_pred_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      prioridad_media  prioridad_min  prioridad_max  distancia_media  \\\n",
      "0            2.009741       2.193658       1.650378         0.910957   \n",
      "1            2.009741       2.193658       1.650378         0.714464   \n",
      "2            2.009741       2.193658       1.650378         0.447251   \n",
      "3            2.009741       2.193658       1.650378         0.182587   \n",
      "4            2.009741       2.193658       1.650378        -0.066193   \n",
      "...               ...            ...            ...              ...   \n",
      "4075         2.009741       2.193658       1.650378        -0.371317   \n",
      "4076         2.009741       2.193658       1.650378        -0.406428   \n",
      "4077         2.009741       2.193658       1.650378        -0.124476   \n",
      "4078         1.415311      -0.754618       1.650378         0.165797   \n",
      "4079         0.820880      -0.754618       1.650378         0.271487   \n",
      "\n",
      "      distancia_min              geometry  predicted_value  final_prediction  \n",
      "0          1.020546  POINT (2.054 41.318)        34.289335             39.05  \n",
      "1          0.789665  POINT (2.054 41.319)        35.351719             39.05  \n",
      "2          0.467639  POINT (2.054 41.321)        36.836688             39.05  \n",
      "3          0.156626  POINT (2.054 41.324)        38.294711             39.05  \n",
      "4         -0.121599  POINT (2.054 41.326)        39.726016             39.05  \n",
      "...             ...                   ...              ...               ...  \n",
      "4075      -0.307358   POINT (2.228 41.46)        66.405551             39.05  \n",
      "4076      -0.355111  POINT (2.228 41.462)        65.202853             39.05  \n",
      "4077      -0.081907  POINT (2.228 41.465)        63.899323             39.05  \n",
      "4078       0.233678  POINT (2.228 41.467)        62.495817             39.05  \n",
      "4079       0.240707  POINT (2.228 41.468)        61.647527             39.05  \n",
      "\n",
      "[4080 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "resultat_categoriques.head()\n",
    "# Realizar predicciones utilizando el modelo entrenado\n",
    "predicciones = rf_model.predict(resultat_categoriques[features])\n",
    "\n",
    "# Mostrar las predicciones\n",
    "resultat_categoriques['final_prediction'] = predicciones\n",
    "final_predict = resultat_categoriques.copy()\n",
    "\n",
    "print(final_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'tocsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[221], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m final \u001b[38;5;241m=\u001b[39m final_predict[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocsv\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./barcelona_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'tocsv'"
     ]
    }
   ],
   "source": [
    "final = final_predict[['geometry', 'final_prediction']].copy()\n",
    "final.to_csv('./barcelona_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 3014538, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 5.502108\n",
      "Nueva concentración: 13.15738127031035\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Cargar y preparar los datos\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "\n",
    "#data = df_combined.sample(n=50000, random_state=42)\n",
    "data = df_combined.copy()\n",
    "data.rename(columns={'Date': 'date', 'no2': 'concentration'}, inplace=True)\n",
    "# Convert 'date' column to datetime format\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "data.dropna(subset=['date'], inplace=True)\n",
    "# Generar características adicionales (temporales y geográficas)\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['day'] = data['date'].dt.day\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "# Variables predictoras y objetivo\n",
    "X = data[['concentration', 'lat', 'lon', 'hour', 'day', 'month']]\n",
    "y = data['concentration'].shift(-1)  # El siguiente valor de concentración\n",
    "\n",
    "# Eliminar valores nulos generados por el shift\n",
    "X = X[:-1]\n",
    "y = y[:-1]\n",
    "\n",
    "# Entrenar el modelo con todos los datos\n",
    "#model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "#model.fit(X, y)\n",
    "#200 0.05, max_depth= 5\n",
    "model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X,y)\n",
    "\n",
    "# Predicción con un nuevo conjunto de datos\n",
    "# Sustituye estos valores por los datos de entrada para la predicción\n",
    "new_data = pd.DataFrame({\n",
    "    'concentration': [0.46611],  # Concentración inicial\n",
    "    'lat': [41.39216],\n",
    "    'lon': [2.009802],\n",
    "    'hour': [0],\n",
    "    'day': [1],\n",
    "    'month': [1]\n",
    "})\n",
    "new_concentration = model.predict(new_data)\n",
    "print(f\"Nueva concentración: {new_concentration[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       concentration       lat       lon                date     id\n",
      "0               24.8  41.39216  2.009802 2023-01-01 00:00:00      1\n",
      "1               24.8  41.39216  2.009802 2023-01-01 01:00:00      2\n",
      "2               24.8  41.39216  2.009802 2023-01-01 02:00:00      3\n",
      "3               24.8  41.39216  2.009802 2023-01-01 03:00:00      4\n",
      "4               24.8  41.39216  2.009802 2023-01-01 04:00:00      5\n",
      "...              ...       ...       ...                 ...    ...\n",
      "35035           37.9  41.32177  2.082141 2023-12-31 19:00:00  35036\n",
      "35036           37.9  41.32177  2.082141 2023-12-31 20:00:00  35037\n",
      "35037           37.9  41.32177  2.082141 2023-12-31 21:00:00  35038\n",
      "35038           37.9  41.32177  2.082141 2023-12-31 22:00:00  35039\n",
      "35039           37.9  41.32177  2.082141 2023-12-31 23:00:00  35040\n",
      "\n",
      "[35040 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "obs_loc = final_predict[['geometry','final_prediction']].copy()\n",
    "obs_loc['lat'] = obs_loc['geometry'].apply(lambda geom: geom.y)\n",
    "obs_loc['lon'] = obs_loc['geometry'].apply(lambda geom: geom.x)\n",
    "obs_loc = obs_loc.drop(columns=['geometry'])\n",
    "obs_loc.rename(columns={'final_prediction': 'concentration'}, inplace=True)\n",
    "\n",
    "# Generate a date range from January 1, 2023 to December 31, 2023, hourly\n",
    "date_range = pd.date_range(start='2023-01-01 00:00:00', end='2023-12-31 23:00:00', freq='H')\n",
    "\n",
    "# Repeat the date range for each point\n",
    "obs_loc = obs_loc.loc[obs_loc.index.repeat(len(date_range))].reset_index(drop=True)\n",
    "\n",
    "# Assign the repeated date range to the 'Date' column\n",
    "obs_loc['date'] = pd.concat([pd.Series(date_range)] * 4, ignore_index=True)\n",
    "\n",
    "# Add an ID column\n",
    "obs_loc['id'] = obs_loc.index + 1\n",
    "\n",
    "print(obs_loc)\n",
    "\n",
    "obs_loc.to_csv('transport_notemps.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def weight_by_hour(hour, peak_hours=[7, 8, 18, 19]):\\n  \\n    # Función sinusoidal suave para crear un perfil de peso\\n   \\n    # Normalizar horas pico\\n    peak_weights = np.exp(-((np.array(peak_hours) - hour)**2) / (4**2))\\n    weight = peak_weights.max()\\n    \\n    return weight\\n\\n# Apply the weight_by_hour function to the 'date' column to generate weights\\nobs_loc['hour_weight'] = obs_loc['date'].dt.hour.apply(weight_by_hour)\\n\\nobs_loc['hour_weight'] += 0.5\\n\\nprint(obs_loc[['date', 'hour_weight']].max())\\n\\nobs_loc['weighted_concentration'] = obs_loc['concentration'] * obs_loc['hour_weight']\\nprint(obs_loc[['date', 'concentration', 'hour_weight', 'weighted_concentration']].head())\\n# Update the 'concentration' column with the 'weighted_concentration' values\\nobs_loc['concentration'] = obs_loc['weighted_concentration']\\n\\n# Drop the 'weighted_concentration' column as it is no longer needed\\nobs_loc.drop(columns=['weighted_concentration', 'hour_weight'], inplace=True)\\n\\nprint(obs_loc.head())\\n\\nobs_loc.to_csv('transport_weighted2.csv', index=False)\""
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def weight_by_hour(hour, peak_hours=[7, 8, 18, 19]):\n",
    "  \n",
    "    # Función sinusoidal suave para crear un perfil de peso\n",
    "   \n",
    "    # Normalizar horas pico\n",
    "    peak_weights = np.exp(-((np.array(peak_hours) - hour)**2) / (4**2))\n",
    "    weight = peak_weights.max()\n",
    "    \n",
    "    return weight\n",
    "\n",
    "# Apply the weight_by_hour function to the 'date' column to generate weights\n",
    "obs_loc['hour_weight'] = obs_loc['date'].dt.hour.apply(weight_by_hour)\n",
    "\n",
    "obs_loc['hour_weight'] += 0.5\n",
    "\n",
    "print(obs_loc[['date', 'hour_weight']].max())\n",
    "\n",
    "obs_loc['weighted_concentration'] = obs_loc['concentration'] * obs_loc['hour_weight']\n",
    "print(obs_loc[['date', 'concentration', 'hour_weight', 'weighted_concentration']].head())\n",
    "# Update the 'concentration' column with the 'weighted_concentration' values\n",
    "obs_loc['concentration'] = obs_loc['weighted_concentration']\n",
    "\n",
    "# Drop the 'weighted_concentration' column as it is no longer needed\n",
    "obs_loc.drop(columns=['weighted_concentration', 'hour_weight'], inplace=True)\n",
    "\n",
    "print(obs_loc.head())\n",
    "\n",
    "obs_loc.to_csv('transport_weighted2.csv', index=False)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   concentration       lat       lon                date  id\n",
      "0           24.8  41.39216  2.009802 2023-01-01 00:00:00   1\n",
      "1           24.8  41.39216  2.009802 2023-01-01 01:00:00   2\n",
      "2           24.8  41.39216  2.009802 2023-01-01 02:00:00   3\n",
      "3           24.8  41.39216  2.009802 2023-01-01 03:00:00   4\n",
      "4           24.8  41.39216  2.009802 2023-01-01 04:00:00   5\n",
      "   concentration       lat       lon                date  id  \\\n",
      "0           24.8  41.39216  2.009802 2023-01-01 00:00:00   1   \n",
      "1           24.8  41.39216  2.009802 2023-01-01 01:00:00   2   \n",
      "2           24.8  41.39216  2.009802 2023-01-01 02:00:00   3   \n",
      "3           24.8  41.39216  2.009802 2023-01-01 03:00:00   4   \n",
      "4           24.8  41.39216  2.009802 2023-01-01 04:00:00   5   \n",
      "\n",
      "   predicted_concentration  \n",
      "0                20.800043  \n",
      "1                20.535820  \n",
      "2                20.535820  \n",
      "3                20.535820  \n",
      "4                19.122662  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(obs_loc.head())\n",
    "# Fit the model for each row in obs_loc\n",
    "obs_loc['predicted_concentration'] = obs_loc.apply(\n",
    "    lambda row: model.predict(pd.DataFrame({\n",
    "        'concentration': [row['concentration']],\n",
    "        'lat': [row['lat']],\n",
    "        'lon': [row['lon']],\n",
    "        'hour': [row['date'].hour],\n",
    "        'day': [row['date'].day],\n",
    "        'month': [row['date'].month]\n",
    "    }))[0], axis=1\n",
    ")\n",
    "\n",
    "print(obs_loc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV guardado como 'predicted_concentration.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'predicted_concentration' en 'concentration'\n",
    "obs_loc['concentration'] = obs_loc['predicted_concentration']\n",
    "\n",
    "# Eliminar la columna 'predicted_concentration'\n",
    "obs_loc.drop(columns=['predicted_concentration'], inplace=True)\n",
    "\n",
    "# Guardar el DataFrame resultante en un archivo CSV\n",
    "obs_loc.to_csv('transport_lgbmsensabans.csv', index=False)\n",
    "\n",
    "print(\"Archivo CSV guardado como 'predicted_concentration.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       concentration       lat       lon                date     id\n",
      "0           7.232036  41.39216  2.009802 2023-01-01 00:00:00      1\n",
      "1           6.921241  41.39216  2.009802 2023-01-01 01:00:00      2\n",
      "2           5.847131  41.39216  2.009802 2023-01-01 02:00:00      3\n",
      "3           6.741950  41.39216  2.009802 2023-01-01 03:00:00      4\n",
      "4           5.681535  41.39216  2.009802 2023-01-01 04:00:00      5\n",
      "...              ...       ...       ...                 ...    ...\n",
      "35035       5.778407  41.32177  2.082141 2023-12-31 19:00:00  35036\n",
      "35036       6.472604  41.32177  2.082141 2023-12-31 20:00:00  35037\n",
      "35037       6.719291  41.32177  2.082141 2023-12-31 21:00:00  35038\n",
      "35038       7.006095  41.32177  2.082141 2023-12-31 22:00:00  35039\n",
      "35039       6.263540  41.32177  2.082141 2023-12-31 23:00:00  35040\n",
      "\n",
      "[35040 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "sddelected_columns = obs_loc[['concentration', 'lat', 'lon', 'date', 'id']]\n",
    "print(selected_columns)\n",
    "obs_loc.to_csv('transport_ppm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from multiprocessing import Pool\\n\\nstations = gpd.GeoDataFrame(df_obs[['Station', 'geometry']].drop_duplicates().reset_index(drop=True), geometry='geometry')\\nstations['buffer'] = stations.geometry.buffer(100)\\n\\n# Leer las carreteras desde el DataFrame\\ncarreteras = df_transport\\n\\n# Crear un índice espacial para las carreteras\\ncarreteras_sindex = carreteras.sindex\\n\\ndef encontrar_tipo_carretera(buffer):\\n    # Encontrar las carreteras que intersectan con el buffer\\n    posibles_intersecciones = list(carreteras_sindex.intersection(buffer.bounds))\\n    carreteras_intersectadas = carreteras.iloc[posibles_intersecciones]\\n    \\n    # Filtrar las carreteras que realmente intersectan con el buffer\\n    intersecciones = carreteras_intersectadas[carreteras_intersectadas.intersects(buffer)]\\n    \\n    if not intersecciones.empty:\\n        intersecciones['buffer_id'] = buffer.name\\n        return intersecciones[['buffer_id', 'sourceid', 'prioridad']]\\n    else:\\n        return pd.DataFrame(columns=['buffer_id', 'sourceid', 'prioridad'])\\n\\n# Procesar los buffers en paralelo\\nwith Pool() as pool:\\n    resultados = pool.map(encontrar_tipo_carretera, stations['buffer'])\\n\\n# Combinar los resultados\\nresultados_df = pd.concat(resultados, ignore_index=True)\\n\\n# Mostrar resultados\\nprint(resultados_df)\""
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from multiprocessing import Pool\n",
    "\n",
    "stations = gpd.GeoDataFrame(df_obs[['Station', 'geometry']].drop_duplicates().reset_index(drop=True), geometry='geometry')\n",
    "stations['buffer'] = stations.geometry.buffer(100)\n",
    "\n",
    "# Leer las carreteras desde el DataFrame\n",
    "carreteras = df_transport\n",
    "\n",
    "# Crear un índice espacial para las carreteras\n",
    "carreteras_sindex = carreteras.sindex\n",
    "\n",
    "def encontrar_tipo_carretera(buffer):\n",
    "    # Encontrar las carreteras que intersectan con el buffer\n",
    "    posibles_intersecciones = list(carreteras_sindex.intersection(buffer.bounds))\n",
    "    carreteras_intersectadas = carreteras.iloc[posibles_intersecciones]\n",
    "    \n",
    "    # Filtrar las carreteras que realmente intersectan con el buffer\n",
    "    intersecciones = carreteras_intersectadas[carreteras_intersectadas.intersects(buffer)]\n",
    "    \n",
    "    if not intersecciones.empty:\n",
    "        intersecciones['buffer_id'] = buffer.name\n",
    "        return intersecciones[['buffer_id', 'sourceid', 'prioridad']]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['buffer_id', 'sourceid', 'prioridad'])\n",
    "\n",
    "# Procesar los buffers en paralelo\n",
    "with Pool() as pool:\n",
    "    resultados = pool.map(encontrar_tipo_carretera, stations['buffer'])\n",
    "\n",
    "# Combinar los resultados\n",
    "resultados_df = pd.concat(resultados, ignore_index=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(resultados_df)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
