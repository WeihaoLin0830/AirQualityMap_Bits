{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from pykrige.uk import UniversalKriging\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "import pyproj\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"../Dades/AMS_Observacions/\"\n",
    "crs_latlon = 'EPSG:4326'  # WGS84\n",
    "crs_utm = \"EPSG:32631\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Station                 Date   no2\n",
      "2       ES1480A  2022-12-31 23:00:00  62.0\n",
      "3       ES1396A  2022-12-31 23:00:00  60.0\n",
      "4       ES1992A  2022-12-31 23:00:00  45.0\n",
      "5       ES0691A  2022-12-31 23:00:00  43.0\n",
      "6       ES0692A  2022-12-31 23:00:00  45.0\n",
      "...         ...                  ...   ...\n",
      "560633  ES2017A  2023-12-31 22:00:00   7.0\n",
      "560634  ES1930A  2023-12-31 22:00:00   1.0\n",
      "560635  ES1948A  2023-12-31 22:00:00   3.0\n",
      "560636  ES1855A  2023-12-31 22:00:00   2.0\n",
      "560637  ES1854A  2023-12-31 22:00:00   1.0\n",
      "\n",
      "[529219 rows x 3 columns]\n",
      "        Station                 Date   no2        lat       lon  \\\n",
      "0       ES1480A  2022-12-31 23:00:00  62.0  41.398762  2.153472   \n",
      "1       ES1396A  2022-12-31 23:00:00  60.0  41.378803  2.133098   \n",
      "2       ES1992A  2022-12-31 23:00:00  45.0  41.387273  2.115661   \n",
      "3       ES0691A  2022-12-31 23:00:00  43.0  41.403716  2.204736   \n",
      "4       ES0692A  2022-12-31 23:00:00  45.0  41.370760  2.114771   \n",
      "...         ...                  ...   ...        ...       ...   \n",
      "529214  ES2017A  2023-12-31 22:00:00   7.0  40.552819  0.529983   \n",
      "529215  ES1930A  2023-12-31 22:00:00   1.0  40.902693  0.809795   \n",
      "529216  ES1948A  2023-12-31 22:00:00   3.0  40.939553  0.831337   \n",
      "529217  ES1855A  2023-12-31 22:00:00   2.0  41.009506  0.912876   \n",
      "529218  ES1854A  2023-12-31 22:00:00   1.0  41.008212  0.831085   \n",
      "\n",
      "                    geometry  \n",
      "0       POINT (2.153 41.399)  \n",
      "1       POINT (2.133 41.379)  \n",
      "2       POINT (2.116 41.387)  \n",
      "3       POINT (2.205 41.404)  \n",
      "4       POINT (2.115 41.371)  \n",
      "...                      ...  \n",
      "529214   POINT (0.53 40.553)  \n",
      "529215   POINT (0.81 40.903)  \n",
      "529216   POINT (0.831 40.94)  \n",
      "529217   POINT (0.913 41.01)  \n",
      "529218  POINT (0.831 41.008)  \n",
      "\n",
      "[529219 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df_no2 = pd.read_csv('../Dades/AMS_Observacions/gene_sconcno2_2023_xvpca_emep_port.csv') \n",
    "df_estacions = pd.read_csv('../Dades/AMS_Observacions/XVPCA_info_sconcno2_2023.csv')\n",
    "\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df_no2['Date'] = pd.to_datetime(df_no2['Date'])\n",
    "\n",
    "# Pivot the dataframe to have each station ID as a column and the Date as the index\n",
    "# columnas_es = [col for col in df_no2.columns if col.startswith('ES')]\n",
    "columnas_es =df_no2.drop(['Date'], axis=1)\n",
    "\n",
    "\n",
    "# Transpose the dataframe to have each station ID as a row and the Date as the column\n",
    "df_no2_transposed = df_no2.set_index('Date').transpose().reset_index()\n",
    "\n",
    "# Melt the dataframe to have a 'Value' column for each hour\n",
    "df_no2_melted = pd.melt(df_no2_transposed, id_vars=['index'], var_name='Date', value_name='no2')\n",
    "\n",
    "# Rename the 'index' column to 'Station'\n",
    "df_no2_melted.rename(columns={'index': 'Station'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_no2_melted.dropna(subset=['no2'], inplace=True)\n",
    "\n",
    "print(df_no2_melted)\n",
    "\n",
    "# Join the melted dataframe with the station information dataframe\n",
    "df_obs = pd.merge(df_no2_melted, df_estacions, left_on='Station', right_on='code', how='inner')\n",
    "\n",
    "# Drop the 'code' column as it is redundant\n",
    "df_obs.drop(columns=['code','type'], inplace=True)\n",
    "df_obs['geometry'] = gpd.points_from_xy(df_obs['lon'], df_obs['lat'], crs=crs_utm)\n",
    "print(df_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sourceid  prioridad                                         geometry\n",
      "0  198794320   0.166667  LINESTRING (1.99488 41.66973, 1.99502 41.66971)\n",
      "1  198935562   0.333333  LINESTRING (2.23387 41.94304, 2.23386 41.94206)\n",
      "2  198944072   0.333333  LINESTRING (1.68342 41.33341, 1.68353 41.33318)\n",
      "3  198965364   0.166667  LINESTRING (2.01923 41.57089, 2.01926 41.57088)\n",
      "4  198965365   0.166667  LINESTRING (2.01857 41.57034, 2.01852 41.57033)\n"
     ]
    }
   ],
   "source": [
    "# Cargar el shapefile\n",
    "archivo_shapefile = \"../Dades/OpenTransportMap/Barcelona/roadlinks_ES511.shp\"\n",
    "gdf_barcelona = gpd.read_file(archivo_shapefile)\n",
    "gdf_girona = gpd.read_file(\"../Dades/OpenTransportMap/Girona/roadlinks_ES512.shp\")\n",
    "gdf_lleida = gpd.read_file(\"../Dades/OpenTransportMap/Lleida/roadlinks_ES513.shp\")\n",
    "gdf_tarragona = gpd.read_file(\"../Dades/OpenTransportMap/Tarragona/roadlinks_ES514.shp\")\n",
    "\n",
    "# Concatenate the GeoDataFrames\n",
    "#gdf = pd.concat([gdf_barcelona, gdf_girona, gdf_lleida, gdf_tarragona], ignore_index=True)\n",
    "\n",
    "gdf = gdf_barcelona\n",
    "\n",
    "\n",
    "# Mostrar las primeras filas de la tabla de atributos\n",
    "#print(gdf['functional'].head())\n",
    "\n",
    "# Obtener los tipos únicos de la columna 'functional'\n",
    "tipos_functional = gdf['functional'].unique()\n",
    "\n",
    "# Asignar valores de prioridad a las categorías\n",
    "valores_prioridad = {\n",
    "    'mainRoad': 6 / 6,\n",
    "    'firstClass': 5 / 6,\n",
    "    'secondClass': 4 / 6,\n",
    "    'thirdClass': 3 / 6,\n",
    "    'fourthClass': 2 / 6,\n",
    "    'fifthClass': 1 / 6,    \n",
    "}\n",
    "\n",
    "# Crear una nueva columna en el GeoDataFrame con los valores de prioridad\n",
    "gdf['prioridad'] = gdf['functional'].map(valores_prioridad)\n",
    "\n",
    "# Crear un nuevo DataFrame con las columnas 'sourceid', 'functional' y 'prioridad'\n",
    "df_transport = gdf[['sourceid', 'prioridad','geometry']].drop_duplicates().reset_index(drop=True)\n",
    "# Simplificar la geometría\n",
    "df_transport['geometry'] = df_transport['geometry'].simplify(tolerance=0.01, preserve_topology=True)\n",
    "\n",
    "print(df_transport.head())\n",
    "\n",
    "# Guardar el GeoDataFrame en un archivo CSV\n",
    "#gdf.to_csv(\"./carreteres.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Date</th>\n",
       "      <th>no2</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES1480A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>41.398762</td>\n",
       "      <td>2.153472</td>\n",
       "      <td>POINT (2.153 41.399)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ES1396A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>41.378803</td>\n",
       "      <td>2.133098</td>\n",
       "      <td>POINT (2.133 41.379)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ES1992A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>41.387273</td>\n",
       "      <td>2.115661</td>\n",
       "      <td>POINT (2.116 41.387)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ES0691A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>41.403716</td>\n",
       "      <td>2.204736</td>\n",
       "      <td>POINT (2.205 41.404)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ES0692A</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>45.0</td>\n",
       "      <td>41.370760</td>\n",
       "      <td>2.114771</td>\n",
       "      <td>POINT (2.115 41.371)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Station                 Date   no2        lat       lon  \\\n",
       "0  ES1480A  2022-12-31 23:00:00  62.0  41.398762  2.153472   \n",
       "1  ES1396A  2022-12-31 23:00:00  60.0  41.378803  2.133098   \n",
       "2  ES1992A  2022-12-31 23:00:00  45.0  41.387273  2.115661   \n",
       "3  ES0691A  2022-12-31 23:00:00  43.0  41.403716  2.204736   \n",
       "4  ES0692A  2022-12-31 23:00:00  45.0  41.370760  2.114771   \n",
       "\n",
       "               geometry  \n",
       "0  POINT (2.153 41.399)  \n",
       "1  POINT (2.133 41.379)  \n",
       "2  POINT (2.116 41.387)  \n",
       "3  POINT (2.205 41.404)  \n",
       "4  POINT (2.115 41.371)  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_caliope = pd.read_csv('../Dades/NO2.csv') \n",
    "\n",
    "df_caliope.rename(columns={'sconcno2': 'no2', 'time': 'Date'}, inplace=True)\n",
    "\n",
    "df_caliope['geometry'] = gpd.points_from_xy(df_caliope['lon'], df_caliope['lat'], crs=crs_utm)\n",
    "\n",
    "df_combined = pd.concat([df_obs, df_caliope], ignore_index=True)\n",
    "\n",
    "# Convert df_combined to a GeoDataFrame\n",
    "df_combined = gpd.GeoDataFrame(df_combined, geometry='geometry', crs=crs_utm)\n",
    "\n",
    "# Display the first few rows to verify the conversion\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max NO2: 34.580633549316055\n",
      "Min NO2: 0.00010403053067096775\n"
     ]
    }
   ],
   "source": [
    "# Agrupar por 'geometry' y calcular la media de 'no2'\n",
    "df_combined_mean = df_combined.groupby('geometry')['no2'].mean().reset_index()\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(\"Max NO2:\", df_combined_mean['no2'].max())\n",
    "print(\"Min NO2:\", df_combined_mean['no2'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               geometry  predicted_value\n",
      "0   POINT (2.01 41.392)         0.466110\n",
      "1  POINT (1.192 41.116)         0.193046\n",
      "2  POINT (2.238 41.444)         0.289827\n",
      "3  POINT (2.082 41.322)         0.389294\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Crear GeoDataFrame con puntos de predicción\n",
    "obs_locations = gpd.GeoDataFrame(\n",
    "    {'geometry': [Point(2.009802, 41.39216), Point(1.191975, 41.11588), \n",
    "                  Point(2.237875, 41.44398), Point(2.082141, 41.32177)]},\n",
    "    crs=crs_utm\n",
    ")\n",
    "\n",
    "# Extraer coordenadas y valores del dataset conocido\n",
    "coords = np.array([(geom.x, geom.y) for geom in df_combined_mean.geometry])\n",
    "values = df_combined_mean['no2'].values\n",
    "\n",
    "# Crear el árbol KD\n",
    "tree = cKDTree(coords)\n",
    "\n",
    "# Función IDW\n",
    "def idw(x, y, tree, coords, values, power=2):\n",
    "    \"\"\"\n",
    "    Interpolación por inverso de la distancia (IDW).\n",
    "    - x, y: Coordenadas de predicción\n",
    "    - tree: KDTree construido con los datos conocidos\n",
    "    - coords: Coordenadas conocidas\n",
    "    - values: Valores conocidos\n",
    "    - power: Peso de la distancia\n",
    "    \"\"\"\n",
    "    # Encontrar distancias y vecinos\n",
    "    distances, idx = tree.query(np.c_[x, y], k=len(coords))\n",
    "    \n",
    "    # Manejo de distancias 0 (puntos coincidentes)\n",
    "    distances[distances == 0] = 1e-10  # Evitar división por cero\n",
    "    \n",
    "    # Calcular pesos inversos a las distancias\n",
    "    weights = 1 / distances**power\n",
    "    \n",
    "    # Normalizar los pesos\n",
    "    weights /= np.sum(weights, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calcular valores interpolados\n",
    "    interpolated_values = np.sum(weights * values[idx], axis=1)\n",
    "    return interpolated_values\n",
    "\n",
    "# Coordenadas de los puntos a predecir\n",
    "prediction_coords = np.array([(geom.x, geom.y) for geom in obs_locations.geometry])\n",
    "x_pred, y_pred = prediction_coords[:, 0], prediction_coords[:, 1]\n",
    "\n",
    "# Aplicar IDW a los puntos\n",
    "obs_locations['predicted_value'] = idw(x_pred, y_pred, tree, coords, values)\n",
    "\n",
    "print(obs_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva concentración: 0.5532452156197503\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Cargar y preparar los datos\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "\n",
    "data = df_combined.sample(n=10000, random_state=42)\n",
    "data.rename(columns={'Date': 'date', 'no2': 'concentration'}, inplace=True)\n",
    "# Convert 'date' column to datetime format\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "data.dropna(subset=['date'], inplace=True)\n",
    "# Generar características adicionales (temporales y geográficas)\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['day'] = data['date'].dt.day\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "# Variables predictoras y objetivo\n",
    "X = data[['concentration', 'lat', 'lon', 'hour', 'day', 'month']]\n",
    "y = data['concentration'].shift(-1)  # El siguiente valor de concentración\n",
    "\n",
    "# Eliminar valores nulos generados por el shift\n",
    "X = X[:-1]\n",
    "y = y[:-1]\n",
    "\n",
    "# Entrenar el modelo con todos los datos\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predicción con un nuevo conjunto de datos\n",
    "# Sustituye estos valores por los datos de entrada para la predicción\n",
    "new_data = pd.DataFrame({\n",
    "    'concentration': [0.46611],  # Concentración inicial\n",
    "    'lat': [41.39216],\n",
    "    'lon': [2.009802],\n",
    "    'hour': [0],\n",
    "    'day': [1],\n",
    "    'month': [1]\n",
    "})\n",
    "new_concentration = model.predict(new_data)\n",
    "print(f\"Nueva concentración: {new_concentration[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11ser\\AppData\\Local\\Temp\\ipykernel_21316\\379389475.py:8: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  date_range = pd.date_range(start='2023-01-01 00:00:00', end='2023-12-31 23:00:00', freq='H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   concentration       lat       lon                date  id\n",
      "0        0.46611  41.39216  2.009802 2023-01-01 00:00:00   1\n",
      "1        0.46611  41.39216  2.009802 2023-01-01 01:00:00   2\n",
      "2        0.46611  41.39216  2.009802 2023-01-01 02:00:00   3\n",
      "3        0.46611  41.39216  2.009802 2023-01-01 03:00:00   4\n",
      "4        0.46611  41.39216  2.009802 2023-01-01 04:00:00   5\n",
      "   concentration       lat       lon                date  id  \\\n",
      "0        0.46611  41.39216  2.009802 2023-01-01 00:00:00   1   \n",
      "1        0.46611  41.39216  2.009802 2023-01-01 01:00:00   2   \n",
      "2        0.46611  41.39216  2.009802 2023-01-01 02:00:00   3   \n",
      "3        0.46611  41.39216  2.009802 2023-01-01 03:00:00   4   \n",
      "4        0.46611  41.39216  2.009802 2023-01-01 04:00:00   5   \n",
      "\n",
      "   predicted_concentration  \n",
      "0                 0.553245  \n",
      "1                 0.333306  \n",
      "2                 0.313140  \n",
      "3                 0.443559  \n",
      "4                 0.313043  \n"
     ]
    }
   ],
   "source": [
    "obs_loc = obs_locations.copy()\n",
    "obs_loc['lat'] = obs_loc.geometry.y\n",
    "obs_loc['lon'] = obs_loc.geometry.x\n",
    "obs_loc = obs_loc.drop(columns=['geometry'])\n",
    "obs_loc.rename(columns={'predicted_value': 'concentration'}, inplace=True)\n",
    "\n",
    "# Generate a date range from January 1, 2023 to December 31, 2023, hourly\n",
    "date_range = pd.date_range(start='2023-01-01 00:00:00', end='2023-12-31 23:00:00', freq='H')\n",
    "\n",
    "# Repeat the date range for each point\n",
    "obs_loc = obs_loc.loc[obs_loc.index.repeat(len(date_range))].reset_index(drop=True)\n",
    "\n",
    "# Assign the repeated date range to the 'Date' column\n",
    "obs_loc['date'] = pd.concat([pd.Series(date_range)] * 4, ignore_index=True)\n",
    "\n",
    "# Add an ID column\n",
    "obs_loc['id'] = obs_loc.index + 1\n",
    "\n",
    "print(obs_loc.head())\n",
    "# Fit the model for each row in obs_loc\n",
    "obs_loc['predicted_concentration'] = obs_loc.apply(\n",
    "    lambda row: model.predict(pd.DataFrame({\n",
    "        'concentration': [row['concentration']],\n",
    "        'lat': [row['lat']],\n",
    "        'lon': [row['lon']],\n",
    "        'hour': [row['date'].hour],\n",
    "        'day': [row['date'].day],\n",
    "        'month': [row['date'].month]\n",
    "    }))[0], axis=1\n",
    ")\n",
    "\n",
    "print(obs_loc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'predicted_concentration'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'predicted_concentration'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convertir la columna 'predicted_concentration' en 'concentration'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m obs_loc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcentration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mobs_loc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_concentration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Eliminar la columna 'predicted_concentration'\u001b[39;00m\n\u001b[0;32m      5\u001b[0m obs_loc\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_concentration\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'predicted_concentration'"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'predicted_concentration' en 'concentration'\n",
    "obs_loc['concentration'] = obs_loc['predicted_concentration']\n",
    "\n",
    "# Eliminar la columna 'predicted_concentration'\n",
    "obs_loc.drop(columns=['predicted_concentration'], inplace=True)\n",
    "\n",
    "# Guardar el DataFrame resultante en un archivo CSV\n",
    "obs_loc.to_csv('concentracio_temps.csv', index=False)\n",
    "\n",
    "print(\"Archivo CSV guardado como 'predicted_concentration.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       concentration       lat       lon                date     id\n",
      "0           0.553245  41.39216  2.009802 2023-01-01 00:00:00      1\n",
      "1           0.333306  41.39216  2.009802 2023-01-01 01:00:00      2\n",
      "2           0.313140  41.39216  2.009802 2023-01-01 02:00:00      3\n",
      "3           0.443559  41.39216  2.009802 2023-01-01 03:00:00      4\n",
      "4           0.313043  41.39216  2.009802 2023-01-01 04:00:00      5\n",
      "...              ...       ...       ...                 ...    ...\n",
      "35035       4.581531  41.32177  2.082141 2023-12-31 19:00:00  35036\n",
      "35036       4.391787  41.32177  2.082141 2023-12-31 20:00:00  35037\n",
      "35037       4.291957  41.32177  2.082141 2023-12-31 21:00:00  35038\n",
      "35038       4.242167  41.32177  2.082141 2023-12-31 22:00:00  35039\n",
      "35039       4.242352  41.32177  2.082141 2023-12-31 23:00:00  35040\n",
      "\n",
      "[35040 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(obs_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from multiprocessing import Pool\\n\\nstations = gpd.GeoDataFrame(df_obs[['Station', 'geometry']].drop_duplicates().reset_index(drop=True), geometry='geometry')\\nstations['buffer'] = stations.geometry.buffer(100)\\n\\n# Leer las carreteras desde el DataFrame\\ncarreteras = df_transport\\n\\n# Crear un índice espacial para las carreteras\\ncarreteras_sindex = carreteras.sindex\\n\\ndef encontrar_tipo_carretera(buffer):\\n    # Encontrar las carreteras que intersectan con el buffer\\n    posibles_intersecciones = list(carreteras_sindex.intersection(buffer.bounds))\\n    carreteras_intersectadas = carreteras.iloc[posibles_intersecciones]\\n    \\n    # Filtrar las carreteras que realmente intersectan con el buffer\\n    intersecciones = carreteras_intersectadas[carreteras_intersectadas.intersects(buffer)]\\n    \\n    if not intersecciones.empty:\\n        intersecciones['buffer_id'] = buffer.name\\n        return intersecciones[['buffer_id', 'sourceid', 'prioridad']]\\n    else:\\n        return pd.DataFrame(columns=['buffer_id', 'sourceid', 'prioridad'])\\n\\n# Procesar los buffers en paralelo\\nwith Pool() as pool:\\n    resultados = pool.map(encontrar_tipo_carretera, stations['buffer'])\\n\\n# Combinar los resultados\\nresultados_df = pd.concat(resultados, ignore_index=True)\\n\\n# Mostrar resultados\\nprint(resultados_df)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from multiprocessing import Pool\n",
    "\n",
    "stations = gpd.GeoDataFrame(df_obs[['Station', 'geometry']].drop_duplicates().reset_index(drop=True), geometry='geometry')\n",
    "stations['buffer'] = stations.geometry.buffer(100)\n",
    "\n",
    "# Leer las carreteras desde el DataFrame\n",
    "carreteras = df_transport\n",
    "\n",
    "# Crear un índice espacial para las carreteras\n",
    "carreteras_sindex = carreteras.sindex\n",
    "\n",
    "def encontrar_tipo_carretera(buffer):\n",
    "    # Encontrar las carreteras que intersectan con el buffer\n",
    "    posibles_intersecciones = list(carreteras_sindex.intersection(buffer.bounds))\n",
    "    carreteras_intersectadas = carreteras.iloc[posibles_intersecciones]\n",
    "    \n",
    "    # Filtrar las carreteras que realmente intersectan con el buffer\n",
    "    intersecciones = carreteras_intersectadas[carreteras_intersectadas.intersects(buffer)]\n",
    "    \n",
    "    if not intersecciones.empty:\n",
    "        intersecciones['buffer_id'] = buffer.name\n",
    "        return intersecciones[['buffer_id', 'sourceid', 'prioridad']]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['buffer_id', 'sourceid', 'prioridad'])\n",
    "\n",
    "# Procesar los buffers en paralelo\n",
    "with Pool() as pool:\n",
    "    resultados = pool.map(encontrar_tipo_carretera, stations['buffer'])\n",
    "\n",
    "# Combinar los resultados\n",
    "resultados_df = pd.concat(resultados, ignore_index=True)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(resultados_df)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
